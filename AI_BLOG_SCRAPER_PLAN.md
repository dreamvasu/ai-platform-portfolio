# AI Blog Scraper Implementation Plan

## Overview

**Goal:** Repurpose the Papers scraper to show latest AI company blogs (model releases, research, benchmarks)

**Why:** Shows you stay current with AI industry developments - way more relevant than academic papers for an ML Platform Engineer role.

---

## Current Infrastructure (Already Built)

âœ… **Paper Scraper Service** (FastAPI, deployed on Cloud Run)
- Location: `backend/services/scraper/`
- Endpoint: https://paper-scraper-434831039257.us-central1.run.app
- Webhook integration to Django backend

âœ… **Django Paper Model**
- Fields: title, abstract, authors, url, source, published_date, category, tags, relevance_score
- API: `/api/papers/`
- Pagination, filtering, sorting built-in

âœ… **Frontend Papers Page**
- Component: `frontend/src/pages/Papers.jsx`
- Features: Category filtering, sorting, search
- Ready to display blog posts

---

## What Needs to Change

### 1. Backend Scraper Service

**Add RSS/Blog Scrapers:**

```python
# backend/services/scraper/app/scrapers/blog_scraper.py

import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime

BLOG_SOURCES = {
    'openai': {
        'type': 'rss',
        'url': 'https://openai.com/blog/rss.xml',
        'category': 'model-release'
    },
    'anthropic': {
        'type': 'web',
        'url': 'https://www.anthropic.com/news',
        'category': 'model-release'
    },
    'google-ai': {
        'type': 'rss',
        'url': 'https://blog.google/technology/ai/rss/',
        'category': 'research'
    },
    'meta-ai': {
        'type': 'web',
        'url': 'https://ai.meta.com/blog/',
        'category': 'research'
    },
    'huggingface': {
        'type': 'rss',
        'url': 'https://huggingface.co/blog/feed.xml',
        'category': 'models'
    },
    'microsoft-ai': {
        'type': 'rss',
        'url': 'https://blogs.microsoft.com/ai/feed/',
        'category': 'products'
    }
}

async def scrape_rss_feed(source_config):
    """Scrape blog posts from RSS feed"""
    feed = feedparser.parse(source_config['url'])
    posts = []

    for entry in feed.entries[:10]:  # Latest 10 posts
        post = {
            'title': entry.title,
            'url': entry.link,
            'published_date': datetime(*entry.published_parsed[:6]),
            'abstract': clean_html(entry.summary),
            'source': source_config['name'],
            'category': source_config['category'],
            'tags': extract_tags(entry.title + entry.summary)
        }
        posts.append(post)

    return posts

async def scrape_web_blog(source_config):
    """Scrape blog posts from web page"""
    response = requests.get(source_config['url'])
    soup = BeautifulSoup(response.content, 'html.parser')

    # Custom parsing logic per site
    # ...

    return posts
```

**Update Main Scraper:**
```python
# backend/services/scraper/app/main.py

@app.post("/scrape")
async def trigger_scrape(request: ScrapeRequest):
    if request.source == "blogs":
        # Scrape all AI company blogs
        results = await scrape_all_blogs()
    elif request.source == "arxiv":
        # Keep arXiv scraper for backwards compat
        results = await scrape_arxiv()
```

### 2. Django Backend

**Update Paper Model Categories:**

```python
# backend/portfolio/models.py

CATEGORY_CHOICES = [
    ('model-release', 'Model Release'),      # GPT-4o, Claude 3.5, Llama 3
    ('research', 'Research & Benchmarks'),   # New papers, benchmark results
    ('products', 'Product Updates'),         # Copilot, ChatGPT features
    ('models', 'Model Hub'),                 # HuggingFace trending
    ('infrastructure', 'ML Infrastructure'), # MLOps, serving, deployment
    ('industry', 'Industry News'),           # Company announcements
]

SOURCE_CHOICES = [
    ('openai', 'OpenAI'),
    ('anthropic', 'Anthropic'),
    ('google', 'Google AI'),
    ('meta', 'Meta AI'),
    ('microsoft', 'Microsoft AI'),
    ('huggingface', 'HuggingFace'),
    ('stability', 'Stability AI'),
    ('cohere', 'Cohere'),
    ('mistral', 'Mistral AI'),
]
```

**Add Management Command:**
```python
# backend/portfolio/management/commands/scrape_blogs.py

from django.core.management.base import BaseCommand
import requests

class Command(BaseCommand):
    def handle(self, *args, **options):
        # Trigger blog scraping
        response = requests.post(
            'http://localhost:8001/scrape',
            json={'source': 'blogs', 'max_results': 50}
        )
        self.stdout.write(f'Scraped {response.json()["papers_added"]} blogs')
```

### 3. Frontend Updates

**Rename & Rebrand:**

```jsx
// frontend/src/pages/Papers.jsx â†’ AIUpdates.jsx

<h1>AI Industry Updates</h1>
<p>Latest model releases, research breakthroughs, and AI company announcements</p>

// Update categories
const categories = [
  { id: 'all', label: 'All Updates' },
  { id: 'model-release', label: 'New Models', icon: 'ðŸš€' },
  { id: 'research', label: 'Research', icon: 'ðŸ”¬' },
  { id: 'products', label: 'Products', icon: 'ðŸ’¼' },
  { id: 'models', label: 'Model Hub', icon: 'ðŸ¤—' },
]

// Update display
<div className="blog-post">
  <div className="flex items-center gap-2 mb-2">
    <img src={getSourceLogo(post.source)} className="w-6 h-6" />
    <span className="font-medium">{post.source_display}</span>
    <span className="text-gray-500">â€¢</span>
    <span className="text-sm text-gray-500">
      {formatDate(post.published_date)}
    </span>
  </div>
  <h3 className="text-xl font-bold mb-2">{post.title}</h3>
  <p className="text-gray-600 mb-4">{post.abstract}</p>
  <a href={post.url} className="text-blue-600 hover:text-blue-800">
    Read on {post.source_display} â†’
  </a>
</div>
```

**Update Navigation:**
```jsx
// frontend/src/components/layout/Navbar.jsx

<Link to="/ai-updates">AI Updates</Link>
```

---

## Implementation Steps

### Phase 1: Backend Scraper (30 mins)
1. âœ… Add `feedparser` to requirements.txt
2. âœ… Create `blog_scraper.py` with RSS/web scrapers
3. âœ… Update main.py to support blog scraping
4. âœ… Test locally: scrape OpenAI, Anthropic, Google blogs
5. âœ… Deploy to Cloud Run

### Phase 2: Django Updates (15 mins)
1. âœ… Update Paper model categories (migration)
2. âœ… Update serializers with new display names
3. âœ… Test API returns blog posts correctly
4. âœ… Deploy backend to Cloud Run

### Phase 3: Frontend Rebrand (15 mins)
1. âœ… Rename Papers.jsx â†’ AIUpdates.jsx
2. âœ… Update categories and filtering
3. âœ… Add company logos/icons
4. âœ… Update navigation
5. âœ… Deploy to Vercel

### Phase 4: Testing (10 mins)
1. âœ… Trigger blog scraping
2. âœ… Verify 20-30 posts appear
3. âœ… Check filtering works
4. âœ… Verify links work

**Total Time:** ~70 minutes

---

## Sample Blog Posts We'll Get

**OpenAI:**
- "Introducing GPT-4o: our fastest and most affordable flagship model"
- "Improving o1-preview's safety and security"

**Anthropic:**
- "Introducing Claude 3.5 Sonnet"
- "Evaluating language model safety & security"

**Google AI:**
- "Gemini 1.5 Pro: Our next-generation model"
- "Project IDX: AI-powered workspace"

**Meta AI:**
- "Llama 3: The most capable openly available LLM"
- "Segment Anything Model 2"

**HuggingFace:**
- "Introducing Transformers 4.40"
- "New models trending this week"

---

## Benefits

**For Your Portfolio:**
- âœ… Shows you follow industry trends
- âœ… Demonstrates awareness of latest models/tools
- âœ… Proves you evaluate new tech for platform decisions
- âœ… Much more relevant than academic papers

**For Interviews:**
- "I built a system to track AI company announcements"
- "I stay current with model releases - here's my feed"
- "When GPT-4o came out, I evaluated it for our platform"

**SEO & Traffic:**
- People searching "latest AI models" find your site
- Demonstrates thought leadership
- More valuable content than academic research

---

## Alternative: Manual Curation (10 mins)

If we want to ship faster, we can:
1. Manually add 10-15 important blog posts to database
2. Ship the frontend now
3. Build scraper automation later

**Pros:** Faster to production
**Cons:** Need manual updates

**Recommendation:** Build the scraper - it's only 1 hour and provides automatic updates.

---

## Success Metrics

- âœ… 30+ blog posts scraped automatically
- âœ… Updated daily/weekly via cron job
- âœ… Categories accurately reflect content
- âœ… All links work and point to original sources
- âœ… Page looks professional and well-designed

---

**Status:** Ready to implement
**Next Step:** Phase 1 - Build blog scrapers
